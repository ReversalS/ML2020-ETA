{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline 流程设计\n",
    "\n",
    "快速搭建一个符合比赛要求的pipeline出来，做好模块划分和设计，方便后面分工协作。\n",
    "\n",
    "---\n",
    "\n",
    "**调试**：\n",
    "\n",
    "1. import更新的调试需要先`restart IPython kernel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question or problem definition (**TODO**)\n",
    "\n",
    "dataset为三元组集合，$\\mathbf{D} = \\{p_i, s_i, e_i\\}_{i=1}^N$，其中$p_i$为路径，$s_i$是出发时间，$e_i$是到达时间。\n",
    "给定一个query $q=(o_q, d_q, s_q, \\hat{p}_q)$，其中$o_q$是出发地，$d_q$是目的地，$s_q$是出发时间，\n",
    "$\\hat{p}_q$是验证集的给出的一段路径\n",
    "\n",
    "首先明确是regression，ETA领域主流做法有两种，模拟轨迹或者直接预测时间，baseline这里先考虑直接获得时间.\n",
    "\n",
    "这里面会遇到很多问题，也是之后要解决的：\n",
    "\n",
    "1. $\\boldsymbol{p}$所代表的一系列特征的选取（不定长的特征提取——初步考虑RNN，很核心的一个问题就是海运是个连续的物理空间，很难表示path）\n",
    "2. 预处理怎么填充\n",
    "3. 模型选取（GBDT, FM, ...）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Startup\n",
    "整体打算是把大的流程用jupyter写，而模块写成单独的.py，所有jupyter文件放在`experiment`目录下，所以要import别的模块需要首先修改系统目录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "e:\\Code\\ML2020-ETA\n"
    }
   ],
   "source": [
    "import sys, os\n",
    "os.chdir('..')\n",
    "print(os.path.abspath('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来就是import一系列需要用的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": "        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "
     },
     "metadata": {}
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Exploration, Engineering and Cleaning\n",
    "这部分要读取数据，转换成模型输入的数据。\n",
    "内部可能可以再拆分成不同的小任务，毕竟特征工程这一块在总工作量中所占比重非常高。\n",
    "\n",
    "所有数据都放在`data`目录下，包括未处理的和处理过的中间结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "port = pd.read_csv('data/port.csv')\n",
    "loading_order_event = pd.read_csv('data/loadingOrderEvent.csv', low_memory=False) # 辅助数据\n",
    "gps_record = pd.read_csv('data/train_dev.csv', low_memory=False, chunksize=None) # 前期先小规模数据（对于大数据量调整chuncksize参数）\n",
    "test_data = pd.read_csv('data/A_testData0531.csv', low_memory=False)\n",
    "# 允许使用外部数据，可能包括补充、修正port坐标的数据，以及不同海域航行特征的数据等等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于**A轮测试数据**，目前看TRACE似乎都只有起止港，目前就按照这个做，后面如果出现了差别再改动也是来得及的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'CNSHK-SIKOP', 'COBUN-HKHKG', 'CNSHK-PKQCT', 'CNSHK-SGSIN', 'CNYTN-RTM', 'HKHKG-FRFOS', 'CNYTN-CAVAN', 'CNSHK-MYTPP', 'CNSHK-LBBEY', 'CNHKG-MXZLO', 'CNSHK-GRPIR', 'CNYTN-NZAKL', 'CNYTN-MXZLO', 'CNSHK-ZADUR', 'CNSHA-PAMIT', 'CNSHK-ESALG', 'CNSHK-CLVAP', 'CNYTN-MTMLA', 'CNYTN-ARENA', 'CNSHA-SGSIN', 'CNYTN-MATNG', 'CNYTN-PAONX'}\n"
    }
   ],
   "source": [
    "# 测试test data里面有哪些起止路线\n",
    "routes = set()\n",
    "for _, row in test_data.iterrows():\n",
    "    if row['TRANSPORT_TRACE'] not in routes:\n",
    "        routes.add(row['TRANSPORT_TRACE'])\n",
    "print(routes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于特征工程，baseline这里尽量从简，之后慢慢改。\n",
    "\n",
    "目前对数据特征的一些总结理解。\n",
    "\n",
    "- 参考一下[正是赛习题讲解](https://bbs.huaweicloud.com/forum/thread-57457-1-1.html)，里面有说历史运单数据和GPS中的运单数据是一一对应的（实际上不是），由loadingOrder关联，但是历史信息由于是手动录入误差可能较大，**建议通过GPS录入**。\n",
    "\n",
    "- 作为baseline尽量保证训练数据和测试数据特征选取一致，方便理解，但是后面肯定是要在训练里面加入更多的东西（大概）\n",
    "\n",
    "---\n",
    "\n",
    "**训练数据**：GPS数据为主，历史运单数据为辅，港口数据<u>可能需要外部补充</u>\n",
    "\n",
    "> 先主要考察carrierName, timestamp, \n",
    "\n",
    "> spatial feature: longtitude, latitude, speed, direction, vesselNextPort, vesselNextPortETA, vesselStatus, TRANSPORT_TRACE\n",
    "\n",
    "> temporal feature: timestamp\n",
    "\n",
    "> data reliability: vesselDatasource\n",
    "\n",
    "关于`vesselStatus`，为AIS船舶航行状态，具体来说为\n",
    "\n",
    "名称 | id\n",
    ":-|:-:\n",
    "under way using engine(动力船在航) | 0\n",
    "under way sailing(帆船在航) | 1\n",
    "at anchor(锚泊) | 2\n",
    "not under command(失控) | 3\n",
    "moored(系泊) | 4\n",
    "contrained by her draft(吃水限制) | 5\n",
    "\n",
    "**测试数据**：loadingOrder, timestamp, longtitude, latitude, speed, direction, carrierName, vesselMMSI, onboardDate, TRANSPORT_TRACE\n",
    "\n",
    "**预测数据**：loadingOrder, timestamp, longtitude, latitude, carrierName, vesselMMSI, onboardDate, **ETA**, **creatDate**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# port data\n",
    "# 索引查找"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "nextport na count: 431\nnextport eta na count: 431\n997\n(1000, 13)\nCNYTN-CLIQQ\nCNYTN-CLIQQ\nCNYTN-CLIQQ\n"
    }
   ],
   "source": [
    "# 关于GPS数据：每个运单表示一次运输的运输单号，不会重复使用，一次运输过程中的多条GPS数据拥有相同的运输单号。船号为运单货物所在的船编号，会重复出现在不同次运输的GPS数据中\n",
    "\n",
    "# GPS data\n",
    "## 时间序列问题的角度（季节？）\n",
    "## speed补充（就近 or 插值）\n",
    "## direction补充（就近 or 插值）\n",
    "## vesselNextport补充\n",
    "print('nextport na count:', gps_record['vesselNextport'].isnull().sum())\n",
    "## vesselNextportETA补充\n",
    "print('nextport eta na count:', gps_record['vesselNextportETA'].isnull().sum())\n",
    "## vesselStatus修改&补充\n",
    "from preprocess import vessel_status\n",
    "gps_record['vesselStatus'].fillna(-1, inplace=True)\n",
    "gps_record['vesselStatus'].apply(lambda x: -1 if x == -1 else vessel_status[x])\n",
    "## vesselDatasource补充\n",
    "from preprocess import vessel_datasource\n",
    "gps_record['vesselDatasource'].apply(lambda x:vessel_datasource[x])\n",
    "## trace只有部分有\n",
    "print(gps_record['TRANSPORT_TRACE'].isnull().sum())\n",
    "print(gps_record.shape)\n",
    "non_nan = gps_record['TRANSPORT_TRACE'].notna().tolist()\n",
    "for index, b in enumerate(non_nan):\n",
    "    if b:\n",
    "        print(gps_record['TRANSPORT_TRACE'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{nan: 0, 'TRANSIT PORT ETD': 1, 'UPDATE SHIPMENT ETA': 2, 'TRANSIT PORT ETA': 3, 'SHIPMENT MIT INBOUND DATE': 4, 'ESTIMATED ARRIVAL TIME TO PORT': 5, 'TRANSIT PORT DECLARATION BEGIN': 6, 'ESTIMATED ARRIVAL TO PORT': 7, 'DISCHARGED': 8, 'ESTIMATED ONBOARD DATE': 9, 'PLANNED PICK UP DATE': 10, 'SHIPMENT ONBOARD DATE': 11, 'RDC ATD': 12, 'CONTAINER LOADED ON BOARD': 13, 'DAILY TRACK AND TRACE': 14, 'TRANSIT PORT ATA': 15, 'ARRIVAL AT CFS OR CY': 16, 'CARGO ARRIVAL AT DESTINATION': 17, 'IMP CUSTOMS CLEARANCE START': 18, 'IMP CUSTOMS CLEARANCE FINISHED': 19, 'TRANSIT PORT CUSTOMS RELEASE': 20, 'ARRIVAL AT PORT': 21, 'TRANSIT PORT ATD': 22, 'PICKED UP': 23}\n"
    }
   ],
   "source": [
    "# event data\n",
    "events = set(loading_order_event['EVENT_CODE'].tolist())\n",
    "event_code = dict(zip(events, range(len(events))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于GPS数据和事件数据之间的联系并不是很清楚，所以下面一部分先通过一定的方式把它们联系起来。\n",
    "\n",
    "目前的发现有：\n",
    "\n",
    "1. GPS的订单号loadingOrderEvent里面不一定有（比方说ZQ464072113491）\n",
    "2. case study 如下\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loadingOrder carrierName                 timestamp   longtitue  \\\n113  XN821466065421      OIEQNT  2019-01-01T00:04:03.000Z  131.929728   \n355  XN821466065421      OIEQNT  2019-01-01T00:07:52.000Z  131.949412   \n566  XN821466065421      OIEQNT  2019-01-01T00:11:58.000Z  131.970683   \n656  XN821466065421      OIEQNT  2019-01-01T00:16:04.000Z  131.991967   \n744  XN821466065421      OIEQNT  2019-01-01T00:19:58.000Z  132.012213   \n859  XN821466065421      OIEQNT  2019-01-01T00:24:10.000Z  132.033833   \n933  XN821466065421      OIEQNT  2019-01-01T00:27:34.000Z  132.051342   \n998  XN821466065421      OIEQNT  2019-01-01T00:31:40.000Z  132.072523   \n\n      latitude   vesselMMSI  speed  direction vesselNextport  \\\n113  31.153467  J1826969247     32       7220     MANZANILLO   \n355  31.160452  J1826969247     32       6650     MANZANILLO   \n566  31.167910  J1826969247     32       6780     MANZANILLO   \n656  31.175338  J1826969247     32       6840     MANZANILLO   \n744  31.182527  J1826969247     31       6780     MANZANILLO   \n859  31.190050  J1826969247     31       6800     MANZANILLO   \n933  31.196137  J1826969247     31       6830     MANZANILLO   \n998  31.203360  J1826969247     31       6870     MANZANILLO   \n\n            vesselNextportETA            vesselStatus vesselDatasource  \\\n113  2019-01-17T12:30:00.000Z  under way using engine      Coastal AIS   \n355  2019-01-17T12:30:00.000Z  under way using engine      Coastal AIS   \n566  2019-01-17T12:30:00.000Z  under way using engine      Coastal AIS   \n656  2019-01-17T12:30:00.000Z  under way using engine      Coastal AIS   \n744  2019-01-17T12:30:00.000Z  under way using engine      Coastal AIS   \n859  2019-01-17T12:30:00.000Z  under way using engine      Coastal AIS   \n933  2019-01-17T12:30:00.000Z  under way using engine      Coastal AIS   \n998  2019-01-17T12:30:00.000Z  under way using engine      Coastal AIS   \n\n    TRANSPORT_TRACE  \n113             NaN  \n355             NaN  \n566             NaN  \n656             NaN  \n744             NaN  \n859             NaN  \n933             NaN  \n998             NaN  \n        loadingOrder              EVENT_CODE    EVENT_LOCATION_ID  \\\n2754  XN821466065421     UPDATE SHIPMENT ETA  SINGAPORE_SINGAPORE   \n2755  XN821466065421   SHIPMENT ONBOARD DATE                CNSHK   \n2782  XN821466065421  ESTIMATED ONBOARD DATE                CNSHK   \n2864  XN821466065421     UPDATE SHIPMENT ETA  RIYADH_SAUDI ARABIA   \n\n     EVENT_CONVOLUTION_DATE  \n2754        2019/1/20 20:45  \n2755        2019/1/12 23:00  \n2782        2019/1/17 20:00  \n2864        2019/1/21 14:00  \n"
    }
   ],
   "source": [
    "# 遍历GPS data\n",
    "selected_lorder = 'XN821466065421'\n",
    "print(gps_record[gps_record['loadingOrder'] == selected_lorder])\n",
    "print(loading_order_event[loading_order_event['loadingOrder'] == selected_lorder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "69\n614\n0       NaN\n1    1020.0\n2     480.0\n3    2160.0\n4    2760.0\nName: timestamp, dtype: float64\nnan\n1020\n480\n2160\n2760\n"
    }
   ],
   "source": [
    "# pandas时间操作以及timestamp情况\n",
    "\n",
    "print((test_data['loadingOrder'] == 'CF946210847851').sum())\n",
    "print((test_data['loadingOrder'] == 'CI265639541482').sum())\n",
    "TIMESTAMP_FORMAT = '%Y-%m-%dT%H:%M:%S.000Z'\n",
    "test_time_delta = pd.to_datetime(test_data['timestamp'][:5], format=TIMESTAMP_FORMAT) - pd.to_datetime(test_data['timestamp'][:5],format=TIMESTAMP_FORMAT).shift()\n",
    "print(test_time_delta.dt.total_seconds())   # 说明不是等长采样..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "见[Ensembling & Stacking Models](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python)一文，不过感觉这里面的处理不是很优雅，总之之后还可以改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "    \n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "    \n",
    "    def feature_importance(self, x, y):\n",
    "        print(self.clf.fit(x, y).feature_importances_)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bittfenv20conda766c41579b2b4f15b06adf3d1d69eb2b",
   "display_name": "Python 3.7.7 64-bit ('tfenv20': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}